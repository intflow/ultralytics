{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization.nn.modules import _utils as quant_nn_utils\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "################################################################################\n",
    "import os\n",
    "import re\n",
    "from typing import List, Callable, Union, Dict\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # 이 줄을 추가하여 F.interpolate 사용 에러 해결\n",
    "import torch.optim as optim\n",
    "from torch.cuda import amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Pytorch Quantization\n",
    "import pytorch_quantization\n",
    "\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from pytorch_quantization.nn.modules import _utils as quant_nn_utils\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization import tensor_quant\n",
    "from absl import logging as quant_logging\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "state_dict = torch.load(\"quant_05271800-calibrated.pth\", map_location=\"cpu\")\n",
    "state_dict\n",
    "model=YOLO(\"quant_05271800-calibrated.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model='quant_05271800-calibrated.pth' should be a *.pt PyTorch model to run this method, but is a different format. PyTorch models can train, val, predict and export, i.e. 'model.train(data=...)', but exported formats like ONNX, TensorRT etc. only support 'predict' and 'val' modes, i.e. 'yolo predict model=yolov8n.onnx'.\nTo run CUDA or MPS inference please pass the device argument directly in your inference command, i.e. 'model.predict(source=..., device=0)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m output_names \u001b[39m=\u001b[39m [ \u001b[39m\"\u001b[39m\u001b[39moutput1\u001b[39m\u001b[39m\"\u001b[39m ]\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m pytorch_quantization\u001b[39m.\u001b[39menable_onnx_export():\n\u001b[1;32m      7\u001b[0m      \u001b[39m# enable_onnx_checker needs to be disabled. See notes below.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m      torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m      9\u001b[0m          model, dummy_input, \u001b[39m\"\u001b[39;49m\u001b[39mquant_05271800-calibrated.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, opset_version\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\n\u001b[1;32m     10\u001b[0m          )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1589\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, (torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule)):\n\u001b[1;32m   1585\u001b[0m     module_typenames_to_export_as_functions \u001b[39m=\u001b[39m _setup_trace_module_map(\n\u001b[1;32m   1586\u001b[0m         model, export_modules_as_functions\n\u001b[1;32m   1587\u001b[0m     )\n\u001b[0;32m-> 1589\u001b[0m \u001b[39mwith\u001b[39;00m exporter_context(model, training, verbose):\n\u001b[1;32m   1590\u001b[0m     val_keep_init_as_ip \u001b[39m=\u001b[39m _decide_keep_init_as_input(\n\u001b[1;32m   1591\u001b[0m         keep_initializers_as_inputs,\n\u001b[1;32m   1592\u001b[0m         operator_export_type,\n\u001b[1;32m   1593\u001b[0m         opset_version,\n\u001b[1;32m   1594\u001b[0m     )\n\u001b[1;32m   1595\u001b[0m     val_add_node_names \u001b[39m=\u001b[39m _decide_add_node_names(\n\u001b[1;32m   1596\u001b[0m         add_node_names, operator_export_type\n\u001b[1;32m   1597\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:179\u001b[0m, in \u001b[0;36mexporter_context\u001b[0;34m(model, mode, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m    177\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexporter_context\u001b[39m(model, mode: _C_onnx\u001b[39m.\u001b[39mTrainingMode, verbose: \u001b[39mbool\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mwith\u001b[39;00m select_model_mode_for_export(\n\u001b[1;32m    180\u001b[0m         model, mode\n\u001b[1;32m    181\u001b[0m     ) \u001b[39mas\u001b[39;00m mode_ctx, disable_apex_o2_state_dict_hook(\n\u001b[1;32m    182\u001b[0m         model\n\u001b[1;32m    183\u001b[0m     ) \u001b[39mas\u001b[39;00m apex_ctx, setup_onnx_logging(\n\u001b[1;32m    184\u001b[0m         verbose\n\u001b[1;32m    185\u001b[0m     ) \u001b[39mas\u001b[39;00m log_ctx, diagnostics\u001b[39m.\u001b[39mcreate_export_diagnostic_context() \u001b[39mas\u001b[39;00m diagnostic_ctx:\n\u001b[1;32m    186\u001b[0m         \u001b[39myield\u001b[39;00m (mode_ctx, apex_ctx, log_ctx, diagnostic_ctx)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:119\u001b[0m, in \u001b[0;36mselect_model_mode_for_export\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    117\u001b[0m         model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m _C_onnx\u001b[39m.\u001b[39mTrainingMode\u001b[39m.\u001b[39mEVAL:\n\u001b[0;32m--> 119\u001b[0m         model\u001b[39m.\u001b[39;49mtrain(\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# else mode == _C_onnx.TrainingMode.PRESERVE, do nothing\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/works/ultralytics/ultralytics/engine/model.py:636\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    605\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    606\u001b[0m     trainer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    607\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    608\u001b[0m ):\n\u001b[1;32m    609\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39m    Trains the model using the specified dataset and training configuration.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39m        ModuleNotFoundError: If the HUB SDK is not installed.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_is_pytorch_model()\n\u001b[1;32m    637\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mid:  \u001b[39m# Ultralytics HUB session with loaded model\u001b[39;00m\n\u001b[1;32m    638\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(kwargs):\n",
      "File \u001b[0;32m/works/ultralytics/ultralytics/engine/model.py:259\u001b[0m, in \u001b[0;36mModel._check_is_pytorch_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m pt_module \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, nn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (pt_module \u001b[39mor\u001b[39;00m pt_str):\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m should be a *.pt PyTorch model to run this method, but is a different format. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch models can train, val, predict and export, i.e. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel.train(data=...)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, but exported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mformats like ONNX, TensorRT etc. only support \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m\u001b[39m modes, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi.e. \u001b[39m\u001b[39m'\u001b[39m\u001b[39myolo predict model=yolov8n.onnx\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTo run CUDA or MPS inference please pass the device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margument directly in your inference command, i.e. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel.predict(source=..., device=0)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: model='quant_05271800-calibrated.pth' should be a *.pt PyTorch model to run this method, but is a different format. PyTorch models can train, val, predict and export, i.e. 'model.train(data=...)', but exported formats like ONNX, TensorRT etc. only support 'predict' and 'val' modes, i.e. 'yolo predict model=yolov8n.onnx'.\nTo run CUDA or MPS inference please pass the device argument directly in your inference command, i.e. 'model.predict(source=..., device=0)'"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 640, 640, device='cuda')\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "with pytorch_quantization.enable_onnx_export():\n",
    "     # enable_onnx_checker needs to be disabled. See notes below.\n",
    "     torch.onnx.export(\n",
    "         model, dummy_input, \"quant_05271800-calibrated.onnx\", verbose=True, opset_version=10\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
